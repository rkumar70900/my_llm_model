# TinyGPT: A Minimalist Implementation of a Transformer-based Language Model

TinyGPT is a small-scale implementation of a transformer-based language model, built from scratch using PyTorch. This educational project demonstrates the core components of modern language models in a concise and understandable way.

## ğŸš€ Features

- **Pure PyTorch Implementation**: Built from the ground up with minimal dependencies
- **Educational Focus**: Clean, well-commented code for learning purposes
- **Modular Design**: Easy to understand and extend
- **Small Footprint**: Designed to run on consumer hardware

## ğŸ—ï¸ Architecture

TinyGPT follows a simplified transformer decoder-only architecture:

```
TinyGPT(
 â”œâ”€ TokenEmbedding
 â”œâ”€ PositionalEmbedding
 â”œâ”€ DecoderBlock Ã— N
 â”‚   â”œâ”€ LayerNorm
 â”‚   â”œâ”€ MultiHeadSelfAttention
 â”‚   â”œâ”€ Linear Projection
 â”‚   â””â”€ FeedForward Network (2 layers, GELU)
 â””â”€ Output Projection
)
```

## ğŸ“¦ Dependencies

- Python 3.6+
- PyTorch
- NumPy

## ğŸ“š File Structure

- `llm_scratch/`
  - `modules_llm.py`: Core transformer components and utilities
  - `final_model.py`: Main model implementation and training loop
  - `my_first_model.py`: Initial implementation (for reference)
  - `test.py`: Testing and evaluation scripts
  - `architecture.txt`: Model architecture overview

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Inspired by the original [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper
- Built for educational purposes

